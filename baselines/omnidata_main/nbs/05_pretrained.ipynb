{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Models + Demo\n",
    "\n",
    "> This page is for downloading and using the pretrained models in PyTorch. You can also try a <a href=//omnidata.vision/demo>demo in your browser</a> or <a href=//docs.omnidata.vision/training.html>train your own models</a>.\n",
    "Short explanation of models: to demonstrate that data is capable of training strong models and not too much limited by rendering, mesh coarseness, etc, we train some models for different tasks. At the time of publishing, the models were comparable or better than sota (link oasis) for common multiple vision tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/EPFL-VILAB/omnidata \n",
    "cd omnidata/omnidata_tools/torch\n",
    "conda create -n testenv -y python=3.8\n",
    "source activate testenv\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "You can see the complete list of required packages in [omnidata-tools/torch/requirements.txt](https://github.com/EPFL-VILAB/omnidata/blob/main/omnidata_tools/torch/requirements.txt). We recommend using virtualenv for the installation.\n",
    "\n",
    "## Pretrained Models\n",
    "You can download our pretrained models for surface normal estimation and depth estimation. For each task there are two versions of the models--a V1 used in the paper, and a stronger V2 released in March 2022.\n",
    "\n",
    "\n",
    "### Network Architecture\n",
    "#### Version 2 models (stronger than V1)\n",
    "These are DPT architectures trained on more data using both [3D Data Augmentations](https://3dcommoncorruptions.epfl.ch) and [Cross-Task Consistency](https://consistency.epfl.ch). Here's the list of updates in Version 2 models:\n",
    "* Surface Normal Prediction:\n",
    "  * New model is based on DPT architecture.\n",
    "  * [Habitat-Matterport 3D Dataset (HM3D)](https://aihabitat.org/datasets/hm3d/) is added to the training data.\n",
    "  * 1 week of training with 2D and [3D data augmentations](https://3dcommoncorruptions.epfl.ch) and 1 week of training with [cross-task consistency](https://consistency.epfl.ch). \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th></th>\n",
    "    <th colspan=\"2\">Angular Error</th>\n",
    "    <th colspan=\"3\">% Within t</th>\n",
    "    <th colspan=\"2\">Relative Normal</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td> Method</td>\n",
    "    <td align=\"center\">Training Data</td>\n",
    "    <td align=\"center\">Mean</td>\n",
    "    <td align=\"center\">Median</td>\n",
    "    <td align=\"center\">11.25</td>\n",
    "    <td align=\"center\">22.5</td>\n",
    "    <td align=\"center\">30</td>\n",
    "    <td align=\"center\">AUC_o</td>\n",
    "    <td align=\"center\">AUC_p</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Hourglass</td>\n",
    "    <td align=\"center\">OASIS</td>\n",
    "      <td align=\"center\"><b>23.91</b></td>\n",
    "    <td align=\"center\">18.16</td>\n",
    "      <td align=\"center\"><b>31.23</b></td>\n",
    "    <td align=\"center\">59.45</td>\n",
    "    <td align=\"center\">71.77</td>\n",
    "    <td align=\"center\">0.5913</td>\n",
    "    <td align=\"center\">0.5786</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>UNet (v1)</td>\n",
    "    <td align=\"center\">Omnidata</td>\n",
    "    <td align=\"center\">24.87</td>\n",
    "      <td align=\"center\"><b>18.04</b></td>\n",
    "      <td align=\"center\">31.02</td>\n",
    "    <td align=\"center\">59.53<br></td>\n",
    "    <td align=\"center\">71.37</td>\n",
    "      <td align=\"center\"><b>0.6692</b></td>\n",
    "    <td align=\"center\">0.6758</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>DPT (v2)</td>\n",
    "    <td align=\"center\">Omnidata</td>\n",
    "    <td align=\"center\">24.16</td>\n",
    "    <td align=\"center\">18.23</td>\n",
    "    <td align=\"center\">27.71</td>\n",
    "      <td align=\"center\"><b>60.95</b><br></td>\n",
    "      <td align=\"center\"><b>74.15</b></td>\n",
    "    <td align=\"center\">0.6646</td>\n",
    "      <td align=\"center\"><b>0.7261</b></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Human (Approx.)</b></td>\n",
    "    <td align=\"center\"><b>-</b></td>\n",
    "    <td align=\"center\"><b>17.27</b></td>\n",
    "    <td align=\"center\"><b>12.92</b></td>\n",
    "    <td align=\"center\"><b>44.36</b></td>\n",
    "    <td align=\"center\"><b>76.16</b></td>\n",
    "    <td align=\"center\"><b>85.24</b></td>\n",
    "    <td align=\"center\"><b>0.8826</b></td>\n",
    "    <td align=\"center\">0.6514</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* Depth Prediction:\n",
    "  * [Habitat-Matterport 3D Dataset (HM3D)](https://aihabitat.org/datasets/hm3d/) and 5 [MiDaS](https://github.com/isl-org/MiDaS) dataset components (RedWebDataset, HRWSIDataset, MegaDepthDataset, TartanAirDataset, BlendedMVS) are added to the training data.\n",
    "  * 1 week of training with 2D and [3D data augmentations](https://3dcommoncorruptions.epfl.ch) and 1 week of training with [cross-task consistency](https://consistency.epfl.ch).\n",
    "\n",
    "\n",
    "#### Version 1 models\n",
    "The surface normal network is based on the [UNet](https://arxiv.org/pdf/1505.04597.pdf) architecture (6 down/6 up). It is trained with both angular and L1 loss and input resolutions between 256 and 512.\n",
    "\n",
    "The depth networks have DPT-based architectures (similar to [MiDaS v3.0](https://github.com/isl-org/MiDaS)) and are trained with scale- and shift-invariant loss and scale-invariant gradient matching term introduced in [MiDaS](https://arxiv.org/pdf/1907.01341v3.pdf), and also [virtual normal loss](https://openaccess.thecvf.com/content_ICCV_2019/papers/Yin_Enforcing_Geometric_Constraints_of_Virtual_Normal_for_Depth_Prediction_ICCV_2019_paper.pdf). You can see a public implementation of the MiDaS loss [here](#midas-implementation). We provide 2 pretrained depth models for both DPT-hybrid and DPT-large architectures with input resolution 384.\n",
    "\n",
    "#### Download pretrained models\n",
    "```bash\n",
    "sh ./tools/download_depth_models.sh\n",
    "sh ./tools/download_surface_normal_models.sh\n",
    "```\n",
    "These will download the pretrained models for `depth` and `normals` to a folder called `./pretrained_models`.\n",
    "\n",
    "## Run our models on your own image\n",
    "After downloading the [pretrained models](#pretrained-models), you can run them on your own image with the following command:\n",
    "```bash\n",
    "python demo.py --task $TASK --img_path $PATH_TO_IMAGE_OR_FOLDER --output_path $PATH_TO_SAVE_OUTPUT\n",
    "```\n",
    "The `--task` flag should be either `normal` or `depth`. To run the script for a `normal` target on an [example image](./assets/demo/test1.png):\n",
    "```bash\n",
    "python demo.py --task normal --img_path assets/demo/test1.png --output_path assets/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  |   |   |   |  |  |  |\n",
    "| :-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|\n",
    "| <img src=\"/omnidata-tools/images/torch/demo/test1.png\" style='max-width: 100%;'/> |  <img src=\"/omnidata-tools/images/torch/demo/test2.png\" style='max-width: 100%;'/> | <img src=\"/omnidata-tools/images/torch/demo/test3.png\" style='max-width: 100%;'/>  | <img src=\"/omnidata-tools/images/torch/demo/test4.png\" style='max-width: 100%;'/>  | <img src=\"/omnidata-tools/images/torch/demo/test5.png\" style='max-width: 100%;'/> | <img src=\"/omnidata-tools/images/torch/demo/test6.png\" style='max-width: 100%;'/> | <img src=\"/omnidata-tools/images/torch/demo/test7.png\" style='max-width: 100%;'/> |\n",
    "| <img src=\"/omnidata-tools/images/torch/demo/test1_normal.png\" style='max-width: 100%;'/> |  <img src=\"/omnidata-tools/images/torch/demo/test2_normal.png\" style='max-width: 100%;'/> | <img src=\"/omnidata-tools/images/torch/demo/test3_normal.png\" style='max-width: 100%;'/>  | <img src=\"/omnidata-tools/images/torch/demo/test4_normal.png\" style='max-width: 100%;'/>  | <img src=\"/omnidata-tools/images/torch/demo/test5_normal.png\" style='max-width: 100%;'/> | <img src=\"/omnidata-tools/images/torch/demo/test6_normal.png\" style='max-width: 100%;'/> | <img src=\"/omnidata-tools/images/torch/demo/test7_normal.png\" style='max-width: 100%;'/> |\n",
    "| <img src=\"/omnidata-tools/images/torch/demo/test1_depth.png\" style='max-width: 100%;'/> |  <img src=\"/omnidata-tools/images/torch/demo/test2_depth.png\" style='max-width: 100%;'/> | <img src=\"/omnidata-tools/images/torch/demo/test3_depth.png\" style='max-width: 100%;'/>  | <img src=\"/omnidata-tools/images/torch/demo/test4_depth.png\" style='max-width: 100%;'/>  | <img src=\"/omnidata-tools/images/torch/demo/test5_depth.png\" style='max-width: 100%;'/> | <img src=\"/omnidata-tools/images/torch/demo/test6_depth.png\" style='max-width: 100%;'/> | <img src=\"/omnidata-tools/images/torch/demo/test7_depth.png\" style='max-width: 100%;'/> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "If you find the code or models useful, please cite our paper:\n",
    "```\n",
    "@inproceedings{eftekhar2021omnidata,\n",
    "  title={Omnidata: A Scalable Pipeline for Making Multi-Task Mid-Level Vision Datasets From 3D Scans},\n",
    "  author={Eftekhar, Ainaz and Sax, Alexander and Malik, Jitendra and Zamir, Amir},\n",
    "  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n",
    "  pages={10786--10796},\n",
    "  year={2021}\n",
    "}\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
