{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "from omnidata_tools.dataset.download import download\n",
    "from fastcore.script import anno_parser\n",
    "import os\n",
    "os.environ[\"COLUMNS\"] = '100'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to download the starter datset\n",
    "\n",
    "> (in one line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download/Installation\n",
    "\n",
    "**`omnitools.download` is a one-line utility for rapidly downloading the starter (& similar) datasets.** For more about the tools themselves (`omnitools.download` and `omnitools.upload`), please see the [dedicated page](/omnidata-tools/omnitools.html).\n",
    "\n",
    "<!-- **_NOTE:_  There's also a complementary `omnitools.upload` that compresses and stores datasets in a compliant format. If you use the omnidata annotator to create a new datset, then `omnitools.upload` might be useful for when you want to distribute that dataset. I.e. other people will be able to use the download tool to download your dataset.** -->\n",
    "\n",
    "To download the starter dataset, make sure that omnidata-tooling is installed and then run the full download command which will prompt you to accept the component licenses to proceed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the following:** (Estimated download time for [_RGB + 1 Task + Masks_]: **1 day**) (_Full dataset_ [30TB]: **5 days**)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Make sure everything is installed\n",
    "sudo apt-get install aria2\n",
    "pip install 'omnidata-tools' # Just to make sure it's installed\n",
    "\n",
    "# Install the 'debug' subset of the Replica and Taskonomy components of the dataset\n",
    "omnitools.download rgb normals point_info \\\n",
    "  --components replica taskonomy \\\n",
    "  --subset debug \\\n",
    "  --dest ./omnidata_starter_dataset/ --agree-all\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://epfl-vilab.github.io/omnidata-tools/images/download_example.jpg\" alt=\"drawing\" style='max-width: 100%;'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "Here are some other examples:\n",
    "\n",
    "Download the full Omnidata dataset and agree to licenses\n",
    "```bash\n",
    "omnitools.download all --components all --subset fullplus \\\n",
    "  --dest ./omnidata_starter_dataset/ \\\n",
    "  --connections_total 40 --agree\n",
    "```\n",
    "\n",
    "Download Taskonomy only:\n",
    "```bash\n",
    "omnitools.download all --components taskonomy --subset fullplus \\\n",
    "  --dest ./omnidata_starter_dataset/ \\\n",
    "  --connections_total 40 --agree\n",
    "```\n",
    "\n",
    "Omnidata but only depth and masks and keep the compressed files\n",
    "```bash\n",
    "omnitools.download rgb depth mask_valid --components all --subset fullplus \\\n",
    "  --dest ./omnidata_starter_dataset/ \\\n",
    "  --connections_total 40 --agree\n",
    "```\n",
    "\n",
    "Download meshes for Clevr\n",
    "```bash\n",
    "omnitools.download mesh --components clevr_simple --subset fullplus \\\n",
    "  --dest ./omnidata_starter_dataset/ \\\n",
    "  --dest_compressed ./omnidata_starter_dataset_compresssed --keep_compressed True \\\n",
    "  --connections_total 40 --agree\n",
    "```\n",
    "\n",
    "Use multiple workers to download Omnidata--this is for worker 7/100, but do a dryrun\n",
    "```bash\n",
    "omnitools.download all --components all --subset fullplus \\\n",
    "  --num_chunk 6 --num_total_chunks 100 \\\n",
    "  --dest ./omnidata_starter_dataset/ \\\n",
    "  --connections_total 40 --agree --dryrun\n",
    "```\n",
    "\n",
    "...you get the idea :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command-line options\n",
    "\n",
    "`omnitools.download` is pretty configurable, and you can choose which comonents/subset/split/tasks to download and extract. The downloader will spawn many workers to then download those compressed files, verify the download against checksums on the server, and unpack them. Here are the available options:\n",
    "\n",
    "```bash\n",
    "> omnitools.download -h\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: omnitools.download [-h] [--subset {debug,tiny,medium,full,fullplus}]\n",
      "                          [--split {train,val,test,all}]\n",
      "                          [--components {all,replica,taskonomy,gso_in_replica,hypersim,blendedmvs,hm3d,clevr_simple,clevr_complex} [{all,replica,taskonomy,gso_in_replica,hypersim,blendedmvs,hm3d,clevr_simple,clevr_complex} ...]]\n",
      "                          [--dest DEST] [--dest_compressed DEST_COMPRESSED]\n",
      "                          [--keep_compressed KEEP_COMPRESSED] [--only_download ONLY_DOWNLOAD]\n",
      "                          [--max_tries_per_model MAX_TRIES_PER_MODEL]\n",
      "                          [--connections_total CONNECTIONS_TOTAL]\n",
      "                          [--connections_per_server_per_download CONNECTIONS_PER_SERVER_PER_DOWNLOAD]\n",
      "                          [--n_workers N_WORKERS] [--num_chunk NUM_CHUNK]\n",
      "                          [--num_total_chunks NUM_TOTAL_CHUNKS] [--ignore_checksum IGNORE_CHECKSUM]\n",
      "                          [--dryrun] [--aria2_uri ARIA2_URI]\n",
      "                          [--aria2_cmdline_opts ARIA2_CMDLINE_OPTS]\n",
      "                          [--aria2_create_server ARIA2_CREATE_SERVER] [--aria2_secret ARIA2_SECRET]\n",
      "                          [--agree_all]\n",
      "                          domains [domains ...]\n",
      "\n",
      "Downloads Omnidata starter dataset. --- The data is stored on the remote server in a compressed\n",
      "format (.tar.gz). This function downloads the compressed and decompresses it. Examples: download rgb\n",
      "normals point_info --components clevr_simple clevr_complex --connections_total 30\n",
      "\n",
      "positional arguments:\n",
      "  domains                                         Domains to download (comma-separated or 'all')\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help                                      show this help message and exit\n",
      "  --subset {debug,tiny,medium,full,fullplus}      Subset to download (default: debug)\n",
      "  --split {train,val,test,all}                    Split to download (default: all)\n",
      "  --components {all,replica,taskonomy,gso_in_replica,hypersim,blendedmvs,hm3d,clevr_simple,clevr_complex} [{all,replica,taskonomy,gso_in_replica,hypersim,blendedmvs,hm3d,clevr_simple,clevr_complex} ...]\n",
      "                                                  Component datasets to download (comma-separated)\n",
      "                                                  (default: all)\n",
      "  --dest DEST                                     Where to put the uncompressed data (default:\n",
      "                                                  uncompressed/)\n",
      "  --dest_compressed DEST_COMPRESSED               Where to download the compressed data (default:\n",
      "                                                  compressed/)\n",
      "  --keep_compressed KEEP_COMPRESSED               Don't delete compressed files after decompression\n",
      "                                                  (default: False)\n",
      "  --only_download ONLY_DOWNLOAD                   Only download compressed data (default: False)\n",
      "  --max_tries_per_model MAX_TRIES_PER_MODEL       Number of times to try to download model if\n",
      "                                                  checksum fails. (default: 3)\n",
      "  --connections_total CONNECTIONS_TOTAL           Number of simultaneous aria2c connections overall\n",
      "                                                  (note: if not using the RPC server, this is per-\n",
      "                                                  worker) (default: 8)\n",
      "  --connections_per_server_per_download CONNECTIONS_PER_SERVER_PER_DOWNLOAD\n",
      "                                                  Number of simulatneous aria2c connections per\n",
      "                                                  server per download. Defaults to\n",
      "                                                  'total_connections' (note: if not using the RPC\n",
      "                                                  server, this is per-worker)\n",
      "  --n_workers N_WORKERS                           Number of workers to use (default: 32)\n",
      "  --num_chunk NUM_CHUNK                           Download the kth slice of the overall dataset\n",
      "                                                  (default: 0)\n",
      "  --num_total_chunks NUM_TOTAL_CHUNKS             Download the dataset in N total chunks. Use with '\n",
      "                                                  --num_chunk' (default: 1)\n",
      "  --ignore_checksum IGNORE_CHECKSUM               Ignore checksum validation (default: False)\n",
      "  --dryrun                                        Keep compressed files even after decompressing\n",
      "                                                  (default: False)\n",
      "  --aria2_uri ARIA2_URI                           Location of aria2c RPC (if None, use CLI)\n",
      "                                                  (default: http://localhost:6800)\n",
      "  --aria2_cmdline_opts ARIA2_CMDLINE_OPTS         Opts to pass to aria2c (default: )\n",
      "  --aria2_create_server ARIA2_CREATE_SERVER       Create a RPC server at aria2_uri (default: True)\n",
      "  --aria2_secret ARIA2_SECRET                     Secret for aria2c RPC (default: )\n",
      "  --agree_all                                     Agree to all license clickwraps. (default: False)\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "argparser = anno_parser(download)\n",
    "argparser.prog = 'omnitools.download'\n",
    "argparser.print_help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "If you find the code or models useful, please cite our paper:\n",
    "```\n",
    "@inproceedings{eftekhar2021omnidata,\n",
    "  title={Omnidata: A Scalable Pipeline for Making Multi-Task Mid-Level Vision Datasets From 3D Scans},\n",
    "  author={Eftekhar, Ainaz and Sax, Alexander and Malik, Jitendra and Zamir, Amir},\n",
    "  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n",
    "  pages={10786--10796},\n",
    "  year={2021}\n",
    "}\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
